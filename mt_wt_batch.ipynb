{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b3eff32-d9fc-4b4f-a304-8bde387c934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from transformers import T5Tokenizer, T5Model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "from termcolor import colored\n",
    "import dataframe_image as dfi\n",
    "import warnings\n",
    "import wandb\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.warn('DelftStack')\n",
    "warnings.warn('Do not show this message')\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "def allDone():\n",
    "    display(Audio(url='https://www.mediacollege.com/downloads/sound-effects/beep/beep-10.wav', autoplay=True))\n",
    "\n",
    "embed_path = 'data_test'\n",
    "result_path = 'predicted_results'\n",
    "wt_mt_path = 'data_test/wt_mt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09e18d67-9b27-4109-a717-d3b14fb08ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_seq = pd.read_csv('./data/sequence_for_embedding_3w.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f7756bf-d1c2-47cd-8c7e-5ce8a9859ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add space between each amino aicds\n",
    "protein_seq['wt_seq'] = protein_seq['wt_seq'].apply(lambda x: ' '.join(x)).apply(\n",
    "        lambda x: re.sub(r\"[UZOB]\", \"X\", x))\n",
    "protein_seq['mt_seq'] = protein_seq['mt_seq'].apply(lambda x: ' '.join(x)).apply(\n",
    "        lambda x: re.sub(r\"[UZOB]\", \"X\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3acd79e2-e661-48f2-b4ce-f03cd975c118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_seq['label'].astype(str)\n",
    "label_names = set(protein_seq['label'])\n",
    "list(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9f1053d-66d9-44ff-95fc-8030db60381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# protein_seq['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd6da22c-9061-452c-baa8-df0e93948683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# protein_seq = protein_seq.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af0eb5e-475e-44b5-9092-2ea6347709d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ba77279-e982-4164-9d2c-f7db0bc8213e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device name: MPS\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "elif torch.has_mps:\n",
    "    torch.cuda.manual_seed(2020)\n",
    "    device = torch.device('mps')\n",
    "    print('Device name: MPS')\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "376c798a-e3dc-48ea-982d-c52acb7b5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\").to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd21734c-87d5-4e48-ab72-2594f0a37bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def get_embedding(protein_seq, start=None, stop=None, save_path=embed_path, device=device):\n",
    "    \n",
    "    xs = []\n",
    "    result = None\n",
    "    count = 0\n",
    "    embed_error_count = 0\n",
    "    protein_seq = protein_seq[start:stop]\n",
    "    data_len = len(protein_seq)\n",
    "\n",
    "    for index, seq in tqdm(protein_seq.iterrows(), total=protein_seq.shape[0]):\n",
    "        s_len = len(seq['wt_seq'].replace(\" \",'')) + 1\n",
    "        aa_index = seq['aa_index']\n",
    "        label = seq['label']\n",
    "        wt_aa = seq['wt']\n",
    "        mt_aa = seq['mt']\n",
    "        wt_seq = seq['wt_seq']\n",
    "        mt_seq = seq['mt_seq']\n",
    "        # AF_DB = seq['AlphaFoldDB']\n",
    "        # PDB = seq['PDB']\n",
    "        # pathogenicity = seq['pathogenicity']\n",
    "        \n",
    "        # add_special_tokens adds extra token at the end of each sequence\n",
    "        # token_encoding = tokenizer.batch_encode_plus([seq['wt_seq'], seq['mt_seq']], add_special_tokens=True, padding=\"longest\")\n",
    "        wt_token_encoding = tokenizer.batch_encode_plus([seq['wt_seq']], add_special_tokens=True, padding=\"longest\")\n",
    "        wt_input_ids      = torch.tensor(wt_token_encoding['input_ids']).to(device)\n",
    "        wt_attention_mask = torch.tensor(wt_token_encoding['attention_mask']).to(device)\n",
    "        \n",
    "        mt_token_encoding = tokenizer.batch_encode_plus([seq['mt_seq']], add_special_tokens=True, padding=\"longest\")\n",
    "        mt_input_ids      = torch.tensor(mt_token_encoding['input_ids']).to(device)\n",
    "        mt_attention_mask = torch.tensor(mt_token_encoding['attention_mask']).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # returns: ( batch-size x max_seq_len_in_minibatch x embedding_dim )\n",
    "            wt_embedding_repr =model(wt_input_ids, attention_mask=wt_attention_mask)\n",
    "            wt_emb = wt_embedding_repr.last_hidden_state[:, :s_len]\n",
    "            wt_emb = wt_emb[:, aa_index, :]\n",
    "            wt = wt_emb.detach().cpu().numpy().squeeze()\n",
    "            \n",
    "            mt_embedding_repr =model(mt_input_ids, attention_mask=mt_attention_mask)\n",
    "            mt_emb = mt_embedding_repr.last_hidden_state[:, :s_len]\n",
    "            mt_emb = mt_emb[:, aa_index, :]\n",
    "            mt = mt_emb.detach().cpu().numpy().squeeze()\n",
    "\n",
    "            # break\n",
    "            # try:\n",
    "            #     emb = emb[:, aa_index, :]\n",
    "            # except:\n",
    "            #     embed_error_count += 1\n",
    "            #     print(f'embedding error: index: {index}, aa_index:{aa_index}, aa_length: {s_len} , error_count:{embed_error_count}')\n",
    "                \n",
    "            # print(aa_index)\n",
    "            # x = emb.detach().cpu().numpy().squeeze()\n",
    "\n",
    "            xs.append({'wt':wt.reshape(1,-1),'mt':mt.reshape(1,-1), 'label':label})\n",
    "            \n",
    "    # Save results\n",
    "    if not os.path.isdir(f'{save_path}'):\n",
    "        os.mkdir(f'{save_path}')\n",
    "            \n",
    "    if start is None:\n",
    "        # result.to_csv(f'{save_path}/emb_({data_len}).csv', index=False)\n",
    "        with open(f'./data_test/emb({data_len}).pkl', 'wb') as f:\n",
    "            pickle.dump(xs, f)\n",
    "    else:\n",
    "        # result.to_csv(f'{save_path}/emb_{stop}.pkl.csv', index=False)\n",
    "        with open(f'{save_path}/emb_{stop}.pkl', 'wb') as f:\n",
    "            pickle.dump(xs, f)\n",
    "    \n",
    "# get_embedding(seq)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e6ba40f-3bf2-4161-81ed-f9e2cbe4c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_seq = pd.read_csv('./data/sequence_for_embedding_3w.csv')\n",
    "# add space between each amino aicds\n",
    "protein_seq['wt_seq'] = protein_seq['wt_seq'].apply(lambda x: ' '.join(x)).apply(\n",
    "        lambda x: re.sub(r\"[UZOB]\", \"X\", x))\n",
    "protein_seq['mt_seq'] = protein_seq['mt_seq'].apply(lambda x: ' '.join(x)).apply(\n",
    "        lambda x: re.sub(r\"[UZOB]\", \"X\", x))\n",
    "protein_seq['label'].astype(str)\n",
    "label_names = set(protein_seq['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "473b2955-a86b-4fad-8791-a93ed819ed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_seq = protein_seq[protein_seq['Length']\n",
    "                                 <=1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "226c3741-1c90-4c5a-9fce-5dc9dacde077",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_seq = protein_seq[5:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e85789c-c52b-4915-bf1e-e5f7da8d1ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_id</th>\n",
       "      <th>aa_index</th>\n",
       "      <th>Length</th>\n",
       "      <th>wt</th>\n",
       "      <th>mt</th>\n",
       "      <th>wt_seq</th>\n",
       "      <th>mt_seq</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>NP_001108220.1</td>\n",
       "      <td>115</td>\n",
       "      <td>173</td>\n",
       "      <td>V</td>\n",
       "      <td>M</td>\n",
       "      <td>M S M S A N T M I F M I L G A S V V M A I A C ...</td>\n",
       "      <td>M S M S A N T M I F M I L G A S V V M A I A C ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>NP_001108220.1</td>\n",
       "      <td>109</td>\n",
       "      <td>173</td>\n",
       "      <td>G</td>\n",
       "      <td>S</td>\n",
       "      <td>M S M S A N T M I F M I L G A S V V M A I A C ...</td>\n",
       "      <td>M S M S A N T M I F M I L G A S V V M A I A C ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>NP_001108220.1</td>\n",
       "      <td>66</td>\n",
       "      <td>173</td>\n",
       "      <td>G</td>\n",
       "      <td>R</td>\n",
       "      <td>M S M S A N T M I F M I L G A S V V M A I A C ...</td>\n",
       "      <td>M S M S A N T M I F M I L G A S V V M A I A C ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>NP_068835.1</td>\n",
       "      <td>45</td>\n",
       "      <td>139</td>\n",
       "      <td>S</td>\n",
       "      <td>F</td>\n",
       "      <td>M E T N V F H L M L C V T S A R T H K S T S L ...</td>\n",
       "      <td>M E T N V F H L M L C V T S A R T H K S T S L ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>NP_006163.1</td>\n",
       "      <td>118</td>\n",
       "      <td>151</td>\n",
       "      <td>L</td>\n",
       "      <td>M</td>\n",
       "      <td>M S S F S T T T V S F L L L L A F Q L L G Q T ...</td>\n",
       "      <td>M S S F S T T T V S F L L L L A F Q L L G Q T ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>NP_006163.1</td>\n",
       "      <td>98</td>\n",
       "      <td>151</td>\n",
       "      <td>G</td>\n",
       "      <td>R</td>\n",
       "      <td>M S S F S T T T V S F L L L L A F Q L L G Q T ...</td>\n",
       "      <td>M S S F S T T T V S F L L L L A F Q L L G Q T ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>NP_006163.1</td>\n",
       "      <td>91</td>\n",
       "      <td>151</td>\n",
       "      <td>Q</td>\n",
       "      <td>R</td>\n",
       "      <td>M S S F S T T T V S F L L L L A F Q L L G Q T ...</td>\n",
       "      <td>M S S F S T T T V S F L L L L A F Q L L G Q T ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>NP_006163.1</td>\n",
       "      <td>64</td>\n",
       "      <td>151</td>\n",
       "      <td>S</td>\n",
       "      <td>R</td>\n",
       "      <td>M S S F S T T T V S F L L L L A F Q L L G Q T ...</td>\n",
       "      <td>M S S F S T T T V S F L L L L A F Q L L G Q T ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>NP_006163.1</td>\n",
       "      <td>28</td>\n",
       "      <td>151</td>\n",
       "      <td>M</td>\n",
       "      <td>T</td>\n",
       "      <td>M S S F S T T T V S F L L L L A F Q L L G Q T ...</td>\n",
       "      <td>M S S F S T T T V S F L L L L A F Q L L G Q T ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>NP_002512.1</td>\n",
       "      <td>94</td>\n",
       "      <td>134</td>\n",
       "      <td>V</td>\n",
       "      <td>F</td>\n",
       "      <td>M D P Q T A P S R A L L L L L F L H L A F L G ...</td>\n",
       "      <td>M D P Q T A P S R A L L L L L F L H L A F L G ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>NP_002512.1</td>\n",
       "      <td>72</td>\n",
       "      <td>134</td>\n",
       "      <td>R</td>\n",
       "      <td>H</td>\n",
       "      <td>M D P Q T A P S R A L L L L L F L H L A F L G ...</td>\n",
       "      <td>M D P Q T A P S R A L L L L L F L H L A F L G ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>NP_002512.1</td>\n",
       "      <td>25</td>\n",
       "      <td>134</td>\n",
       "      <td>R</td>\n",
       "      <td>L</td>\n",
       "      <td>M D P Q T A P S R A L L L L L F L H L A F L G ...</td>\n",
       "      <td>M D P Q T A P S R A L L L L L F L H L A F L G ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>NP_001083060.1</td>\n",
       "      <td>53</td>\n",
       "      <td>91</td>\n",
       "      <td>Y</td>\n",
       "      <td>C</td>\n",
       "      <td>M G L E D E Q K M L T E S G D P E E E E E E E ...</td>\n",
       "      <td>M G L E D E Q K M L T E S G D P E E E E E E E ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>NP_000540.2</td>\n",
       "      <td>75</td>\n",
       "      <td>138</td>\n",
       "      <td>R</td>\n",
       "      <td>G</td>\n",
       "      <td>M T A L F L M S M L F G L T C G Q A M S F C I ...</td>\n",
       "      <td>M T A L F L M S M L F G L T C G Q A M S F C I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>NP_000540.2</td>\n",
       "      <td>99</td>\n",
       "      <td>138</td>\n",
       "      <td>V</td>\n",
       "      <td>A</td>\n",
       "      <td>M T A L F L M S M L F G L T C G Q A M S F C I ...</td>\n",
       "      <td>M T A L F L M S M L F G L T C G Q A M S F C I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             gene_id  aa_index  Length wt mt  \\\n",
       "45    NP_001108220.1       115     173  V  M   \n",
       "46    NP_001108220.1       109     173  G  S   \n",
       "47    NP_001108220.1        66     173  G  R   \n",
       "193      NP_068835.1        45     139  S  F   \n",
       "307      NP_006163.1       118     151  L  M   \n",
       "308      NP_006163.1        98     151  G  R   \n",
       "309      NP_006163.1        91     151  Q  R   \n",
       "310      NP_006163.1        64     151  S  R   \n",
       "311      NP_006163.1        28     151  M  T   \n",
       "312      NP_002512.1        94     134  V  F   \n",
       "313      NP_002512.1        72     134  R  H   \n",
       "314      NP_002512.1        25     134  R  L   \n",
       "364   NP_001083060.1        53      91  Y  C   \n",
       "1123     NP_000540.2        75     138  R  G   \n",
       "1124     NP_000540.2        99     138  V  A   \n",
       "\n",
       "                                                 wt_seq  \\\n",
       "45    M S M S A N T M I F M I L G A S V V M A I A C ...   \n",
       "46    M S M S A N T M I F M I L G A S V V M A I A C ...   \n",
       "47    M S M S A N T M I F M I L G A S V V M A I A C ...   \n",
       "193   M E T N V F H L M L C V T S A R T H K S T S L ...   \n",
       "307   M S S F S T T T V S F L L L L A F Q L L G Q T ...   \n",
       "308   M S S F S T T T V S F L L L L A F Q L L G Q T ...   \n",
       "309   M S S F S T T T V S F L L L L A F Q L L G Q T ...   \n",
       "310   M S S F S T T T V S F L L L L A F Q L L G Q T ...   \n",
       "311   M S S F S T T T V S F L L L L A F Q L L G Q T ...   \n",
       "312   M D P Q T A P S R A L L L L L F L H L A F L G ...   \n",
       "313   M D P Q T A P S R A L L L L L F L H L A F L G ...   \n",
       "314   M D P Q T A P S R A L L L L L F L H L A F L G ...   \n",
       "364   M G L E D E Q K M L T E S G D P E E E E E E E ...   \n",
       "1123  M T A L F L M S M L F G L T C G Q A M S F C I ...   \n",
       "1124  M T A L F L M S M L F G L T C G Q A M S F C I ...   \n",
       "\n",
       "                                                 mt_seq  label  \n",
       "45    M S M S A N T M I F M I L G A S V V M A I A C ...      1  \n",
       "46    M S M S A N T M I F M I L G A S V V M A I A C ...      0  \n",
       "47    M S M S A N T M I F M I L G A S V V M A I A C ...      1  \n",
       "193   M E T N V F H L M L C V T S A R T H K S T S L ...      0  \n",
       "307   M S S F S T T T V S F L L L L A F Q L L G Q T ...      0  \n",
       "308   M S S F S T T T V S F L L L L A F Q L L G Q T ...      0  \n",
       "309   M S S F S T T T V S F L L L L A F Q L L G Q T ...      0  \n",
       "310   M S S F S T T T V S F L L L L A F Q L L G Q T ...      0  \n",
       "311   M S S F S T T T V S F L L L L A F Q L L G Q T ...      0  \n",
       "312   M D P Q T A P S R A L L L L L F L H L A F L G ...      0  \n",
       "313   M D P Q T A P S R A L L L L L F L H L A F L G ...      0  \n",
       "314   M D P Q T A P S R A L L L L L F L H L A F L G ...      0  \n",
       "364   M G L E D E Q K M L T E S G D P E E E E E E E ...      0  \n",
       "1123  M T A L F L M S M L F G L T C G Q A M S F C I ...      0  \n",
       "1124  M T A L F L M S M L F G L T C G Q A M S F C I ...      0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea6076bd-3a67-41eb-8f4e-f5c8b38ce103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_in_batch(protein_seq, amount):\n",
    "    value_input = amount\n",
    "    data_len = len(protein_seq)\n",
    "    fold = data_len // value_input\n",
    "    remainder = data_len - data_len % value_input\n",
    "\n",
    "    for i in range(fold):\n",
    "        get_embedding(protein_seq,  start = i* value_input, stop = (i+1)*value_input)\n",
    "    \n",
    "    get_embedding(protein_seq, start = remainder, stop = data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cd37785-5279-4dc9-851e-c4ff228e70c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.71it/s]\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:01<00:00,  3.72it/s]\n"
     ]
    }
   ],
   "source": [
    "embed_in_batch(protein_seq,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1626ee4-bafc-4075-93b8-cd48542d1500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "927e6fd4-f585-4155-9708-960701dda4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def data_for_downstream():\n",
    "    path = os.getcwd() + '/data_test/'\n",
    "    concat = []\n",
    "    for pkl in os.listdir(path):\n",
    "        if(\".pkl\" in pkl):\n",
    "            file_path = path + pkl\n",
    "            with open(file_path, 'rb') as file:\n",
    "                y = pickle.load(file)\n",
    "                concat += y\n",
    "    data_y = []\n",
    "    data_wt = []\n",
    "    data_mt = []\n",
    "    for i in range(len(concat)):\n",
    "        data_wt.append(concat[i]['wt'][0])\n",
    "        data_mt.append(concat[i]['mt'][0])\n",
    "        data_y.append(int(concat[i]['label']))\n",
    "    data_wt = np.array(data_wt)\n",
    "    data_mt = np.array(data_mt)\n",
    "    data_X = np.hstack((data_wt,data_mt))\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c274cea3-24f2-457d-b2b9-84850c2de23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() + '/data_test/'\n",
    "concat = []\n",
    "for pkl in os.listdir(path):\n",
    "    if(\".pkl\" in pkl):\n",
    "        file_path = path + pkl\n",
    "        with open(file_path, 'rb') as file:\n",
    "            y = pickle.load(file)\n",
    "            concat += y\n",
    "data_y = []\n",
    "data_wt = []\n",
    "data_mt = []\n",
    "for i in range(len(concat)):\n",
    "    data_wt.append(concat[i]['wt'][0])\n",
    "    data_mt.append(concat[i]['mt'][0])\n",
    "    data_y.append(int(concat[i]['label']))\n",
    "data_wt = np.array(data_wt)\n",
    "data_mt = np.array(data_mt)\n",
    "data_X = np.hstack((data_wt,data_mt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7cd8185e-6631-4a9a-8d50-954c65eee4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b1ce0ee5-2d96-4736-9f89-b6e1fb858166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 2048)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d1b1bf7-06e1-4272-b1f7-b8e5d0cfa396",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = []\n",
    "data_wt = []\n",
    "data_mt = []\n",
    "for i in range(len(y)):\n",
    "    data_wt.append(y[i]['wt'][0])\n",
    "    data_mt.append(y[i]['mt'][0])\n",
    "    data_y.append(int(y[i]['label']))\n",
    "data_wt = np.array(data_wt)\n",
    "data_mt = np.array(data_mt)\n",
    "data_X = np.hstack((data_wt,data_mt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d807fbc-9733-4ee7-b8b0-bb44123842e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2048)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fead9bd4-9151-495d-a1a4-b73b144a50d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:38:50] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# eval_s = [(X_train, y_train), (X_test, y_test)]\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(data_X, data_y)\n",
    "y_xgb = xgb.predict(data_X)\n",
    "accuracy = (y_xgb - data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "135ccd66-e3fe-4093-9e44-4e81a3dbf07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1cbd12-21d2-4fb6-94fe-0791f6417dba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Mt_Wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9f2780a-3ea5-4bef-8892-f0e45ac9ee80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embedding(protein_seq, tokenizer= tokenizer, model = model, start=None, stop=None, input_type=None, device=device, save_path=embed_path, wt_mt_path = wt_mt_path ):\n",
    "    tokenizer = tokenizer\n",
    "    model = model\n",
    "\n",
    "    xs = []\n",
    "    result = None\n",
    "    count = 0\n",
    "    embed_error_count = 0\n",
    "    protein_seq = protein_seq[start:stop]\n",
    "    data_len = len(protein_seq)\n",
    "\n",
    "    for index, seq in tqdm(protein_seq.iterrows(), total=protein_seq.shape[0]):\n",
    "\n",
    "        s_len = len(seq['wt_seq'].replace(\" \", '')) + 1\n",
    "        aa_index = seq['aa_index']\n",
    "        label = seq['label']\n",
    "        wt_aa = seq['wt']\n",
    "        mt_aa = seq['mt']\n",
    "        wt_seq = seq['wt_seq']\n",
    "        mt_seq = seq['mt_seq']\n",
    "        # AF_DB = seq['AlphaFoldDB']\n",
    "        # PDB = seq['PDB']\n",
    "        # pathogenicity = seq['pathogenicity']\n",
    "\n",
    "        if input_type is None:\n",
    "            token_encoding = tokenizer.batch_encode_plus([seq['wt_seq'], seq['mt_seq']], add_special_tokens=True,\n",
    "                                                         padding=\"longest\")\n",
    "        else:\n",
    "            if input_type == 'wt':\n",
    "                input_seq = seq['wt_seq']\n",
    "            elif input_type == 'mt':\n",
    "                input_seq = seq['mt_seq']\n",
    "            else:\n",
    "                print(\"type can be either 'wt or 'mt'. \")\n",
    "            # add_special_tokens adds extra token at the end of each sequence\n",
    "            token_encoding = tokenizer.batch_encode_plus([input_seq], add_special_tokens=True,\n",
    "                                                         padding=\"longest\")\n",
    "            \n",
    "\n",
    "        input_ids = torch.tensor(token_encoding['input_ids']).to(device)\n",
    "        # print('input_ids:', input_ids.shape)\n",
    "        attention_mask = torch.tensor(\n",
    "            token_encoding['attention_mask']).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # returns: ( batch-size x max_seq_len_in_minibatch x embedding_dim )\n",
    "            embedding_repr = model(\n",
    "                input_ids, attention_mask=attention_mask)\n",
    "            emb = embedding_repr.last_hidden_state[:, :s_len]\n",
    "\n",
    "            try:\n",
    "                emb = emb[:, aa_index,:]\n",
    "                \n",
    "            except Exception as e:\n",
    "                os.system('tput bel')\n",
    "                print(e)\n",
    "                embed_error_count += 1\n",
    "                print(\n",
    "                    f'embedding error: index: {index}, aa_index:{aa_index}, aa_length: {s_len} , error_count:{embed_error_count}')\n",
    "\n",
    "            # print(aa_index)\n",
    "            x = emb.detach().cpu().numpy().squeeze()\n",
    "            xs.append({'x': x.reshape(1,-1), 'label':label})\n",
    "            \n",
    "    # Save results\n",
    "    if input_type is None:\n",
    "        # create a folder to save embeddings (large GPU)\n",
    "        if not os.path.isdir(f'{save_path}'):\n",
    "            os.mkdir(f'{save_path}')\n",
    "        \n",
    "        if start is None:\n",
    "        # result.to_csv(f'{save_path}/sequence_embeddings({data_len}).csv', index=False)\n",
    "            with open(f'./{save_path}/emb({data_len}).pkl', 'wb') as f:\n",
    "                pickle.dump(xs, f)\n",
    "        else:\n",
    "            with open(f'./{save_path}/emb_{stop}.pkl', 'wb') as f:\n",
    "                pickle.dump(xs, f)\n",
    "    else:\n",
    "        if start is None:\n",
    "        # result.to_csv(f'{save_path}/sequence_embeddings({data_len}).csv', index=False)\n",
    "            with open(f'./{save_path}/emb({data_len})_{input_type}.pkl', 'wb') as f:\n",
    "                pickle.dump(xs, f)\n",
    "        else:\n",
    "            if not os.path.isdir(f'./{wt_mt_path}'):\n",
    "                os.mkdir(f'./{wt_mt_path}')\n",
    "        # result.to_csv(f'{save_path}/sequence_{stop}_embeddings.csv', index=False)\n",
    "            with open(f'./{wt_mt_path}/{stop}_emb_{input_type}.pkl', 'wb') as f:\n",
    "                pickle.dump(xs, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a11c5b58-fb89-44f5-903c-8a9338b36a2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "protein_seq = pd.read_csv('./data/sequence_for_embedding_3w.csv')\n",
    "# add space between each amino aicds\n",
    "protein_seq['wt_seq'] = protein_seq['wt_seq'].apply(lambda x: ' '.join(x)).apply(\n",
    "        lambda x: re.sub(r\"[UZOB]\", \"X\", x))\n",
    "protein_seq['mt_seq'] = protein_seq['mt_seq'].apply(lambda x: ' '.join(x)).apply(\n",
    "        lambda x: re.sub(r\"[UZOB]\", \"X\", x))\n",
    "protein_seq['label'].astype(str)\n",
    "label_names = set(protein_seq['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b937d95-bbe1-45aa-93bb-7450f412d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_seq = protein_seq[protein_seq['Length']\n",
    "                                 <=200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3edc8149-ee93-4d33-806f-8d420f9b9bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_seq = protein_seq[5:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dec752f8-24b2-4d57-835b-192564437189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_in_batch(protein_seq, amount, input_type = None):\n",
    "    value_input = amount\n",
    "    data_len = len(protein_seq)\n",
    "    fold = data_len // value_input\n",
    "    remainder = data_len - data_len % value_input\n",
    "    input_type = input_type\n",
    "\n",
    "    for i in range(fold):\n",
    "        get_embedding(protein_seq, input_type = input_type,  start = i* value_input, stop = (i+1)*value_input)\n",
    "    \n",
    "    get_embedding(protein_seq, input_type=input_type, start = remainder, stop = data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22ca96bb-eb7f-49ee-ab12-ea035b1e25e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_embedding(protein_seq,input_type = 'wt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0512cfcc-e7be-4ff5-ba3d-7d67379858a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  7.64it/s]\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:00<00:00,  9.21it/s]\n"
     ]
    }
   ],
   "source": [
    "embed_in_batch(protein_seq,10, input_type='wt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d781c9c-a872-4364-8dc2-365d526900be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fecd033-ccbc-45ae-8656-769a633f1aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b89239df-747a-4a14-89c7-1c6aff3be45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_test/wt_mt/emb_10_mt.pkl', 'rb') as file:\n",
    "    y_10_mt = pickle.load(file)\n",
    "with open('./data_test/wt_mt/emb_10_wt.pkl', 'rb') as file:\n",
    "    y_10_wt = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ef3f881-3bf4-4fdf-8c63-4d7a063924dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mt = []\n",
    "data_wt = []\n",
    "for i in range(len(y_10_mt)):\n",
    "    data_mt.append(y_10_mt[i]['x'][0])\n",
    "for i in range(len(y_10_wt)):\n",
    "    data_wt.append(y_10_wt[i]['x'][0])\n",
    "data_mt = np.array(data_mt)\n",
    "data_wt = np.array(data_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9d9932e5-f9fe-499b-abc0-973d9f160419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 1024), (10, 2048))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_wt.shape, data_mt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f88ce63e-c66f-470d-bee7-9b7c0acb6661",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.hstack((data_wt,data_mt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "51c4781a-6c22-4561-9c6c-e0cdf47326c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2048)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8c870191-0f03-4cbb-a2a0-adb9c3a07d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_for_downstream():\n",
    "    path = os.getcwd() + '/data_test/wt_mt/'\n",
    "    data_mt = []\n",
    "    data_wt = []\n",
    "    for pkl in os.listdir(path):\n",
    "        if(\".pkl\" in pkl):\n",
    "            file_path = path + pkl\n",
    "            with open(file_path, 'rb') as file:\n",
    "                y = pickle.load(file)\n",
    "                data_mt += y\n",
    "    data_X = []\n",
    "    data_y = []\n",
    "    for i in range(len(data_mt)):\n",
    "        data_X.append(data_mt[i]['x'][0])\n",
    "        data_y.append(int(data_mt[i]['label']))\n",
    "    data_X = np.array(data_X)\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a98b4d34-e021-4bb6-b19e-8dc850966fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_for_downstream():\n",
    "    path = os.getcwd() + '/data_test/wt_mt/'\n",
    "    concat_mt = []\n",
    "    concat_wt = []\n",
    "    concat = []\n",
    "    for pkl in os.listdir(path):\n",
    "        if(\"mt.pkl\" in pkl):\n",
    "            file_path = path + pkl\n",
    "            with open(file_path, 'rb') as file:\n",
    "                y = pickle.load(file)\n",
    "                concat_mt += y\n",
    "        if(\"wt.pkl\" in pkl):\n",
    "            file_path = path + pkl\n",
    "            with open(file_path, 'rb') as file:\n",
    "                y = pickle.load(file)\n",
    "                concat_wt += y\n",
    "        if(\"wt.pkl\" not in pkl) & (\"mt.pkl\" not in pkl) & (\".pkl\" in pkl):\n",
    "            file_path = path + pkl\n",
    "            with open(file_path, 'rb') as file:\n",
    "                y = pickle.load(file)\n",
    "                concat += y\n",
    "    if len(concat) > 0:\n",
    "        data_X = []\n",
    "        data_y = []\n",
    "        for i in range(len(concat)):\n",
    "            data_X.append(concat[i]['x'][0])\n",
    "            data_y.append(int(concat[i]['label']))\n",
    "        data_X = np.array(data_X)\n",
    "    \n",
    "    else:\n",
    "        data_y = []\n",
    "        data_wt = []\n",
    "        data_mt = []\n",
    "        for i in range(len(concat_mt)):\n",
    "            data_wt.append(concat_wt[i]['x'][0])\n",
    "            data_mt.append(concat_mt[i]['x'][0])\n",
    "            data_y.append(int(concat_mt[i]['label']))\n",
    "        data_wt = np.array(data_wt)\n",
    "        data_mt = np.array(data_mt)\n",
    "        data_X = np.hstack((data_wt,data_mt))\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "306b0724-28a7-44ad-aa2a-f243ac1e2865",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X, data_y = data_for_downstream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "409d1e9c-3edd-426e-b246-1e4df568ef85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15, 2048), 15)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_X.shape, len(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a06fd3dd-aa38-4deb-b164-9daa03889da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mt: 10_emb_mt.pkl\n",
      "mt: 15_emb_mt.pkl\n"
     ]
    }
   ],
   "source": [
    "test_path = os.getcwd() + '/data_test/wt_mt/'\n",
    "concat_mt = []\n",
    "files = os.listdir(test_path)\n",
    "files.sort()\n",
    "for pkl in files:\n",
    "    if(\"mt.pkl\" in pkl):\n",
    "        print('mt:', pkl)\n",
    "        concat_mt += pkl\n",
    "    # if(\"wt.pkl\" in pkl):\n",
    "    #     print('wt:', pkl)\n",
    "    #     concat_wt += pkl\n",
    "    # if(\"wt.pkl\" not in pkl) & (\"mt.pkl\" not in pkl) & (\".pkl\" in pkl):\n",
    "    #     print('pkl file:', pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "692e4868-ac9e-4c27-91fe-3709a6586686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = os.listdir('./data_test/wt_mt/').sort()\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "253971fd-162c-49c9-bb40-4b24cc06e8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f4aef73-1e37-4a6b-beca-852b48a13036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wt: 15_emb_wt.pkl\n",
      "wt: 10_emb_wt.pkl\n"
     ]
    }
   ],
   "source": [
    "test_path = os.getcwd() + '/data_test/wt_mt/'\n",
    "concat_wt = []\n",
    "for pkl in os.listdir(test_path):\n",
    "    if(\"wt.pkl\" in pkl):\n",
    "        print('wt:', pkl)\n",
    "        concat_wt += pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "672d6076-7939-42d8-8b83-be2bff0cacb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10_emb_mt.pkl15_emb_mt.pkl'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(concat_mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26624afa-fc2c-4c78-a517-afaad304cad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15_emb_wt.pkl10_emb_wt.pkl'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(concat_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ea0866a-ebfe-4e41-955c-fa87af69ee2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:13:50] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# eval_s = [(X_train, y_train), (X_test, y_test)]\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(data_X, data_y)\n",
    "y_xgb = xgb.predict(data_X)\n",
    "accuracy = (y_xgb - data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06c29576-5d4a-4f83-9e6e-9ba99c291ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0, -1,  0, -1,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63d137f7-0e61-4ad7-9124-8823e1e497ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d083b65a-f470-46d2-8917-3c17ffdc77ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5cb81f-9351-4213-9237-9a3be42dfa63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
