{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b171d86b-37d0-47f2-925b-20fdc58666ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "\n",
    "import re\n",
    "import random\n",
    "from termcolor import colored\n",
    "# import dataframe_image as dfi\n",
    "# import warnings\n",
    "# import wandb\n",
    "# warnings.filterwarnings('ignore')\n",
    "# warnings.warn('DelftStack')\n",
    "# warnings.warn('Do not show this message')\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "def allDone():\n",
    "    display(Audio(url='https://www.mediacollege.com/downloads/sound-effects/beep/beep-10.wav', autoplay=True))\n",
    "\n",
    "embed_path = 'data_test'\n",
    "result_path = 'ML_predicted_results'\n",
    "wt_mt_path = 'data_test/wt_mt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5e58626-8f65-4aa2-9bab-5e7eb4170a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_dataset(situation, mode):\n",
    "    if 'imbalance' in situation:\n",
    "        mode_train = np.load(f'../data/{situation}/For_ML/{mode}_train.npy')\n",
    "        mode_test = np.load(f'../data/{situation}/For_ML/{mode}_test.npy')\n",
    "\n",
    "        X_train = mode_train[:,:-1]\n",
    "        y_train = mode_train[:,-1].astype(int).tolist()\n",
    "        X_test = mode_test[:,:-1]\n",
    "        y_test = mode_test[:,-1].astype(int).tolist()\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    else:\n",
    "        mode_train1 = np.load(f'../data/{situation}/For_ML/{mode}_train_1.npy')\n",
    "        mode_train2 = np.load(f'../data/{situation}/For_ML/{mode}_train_2.npy')\n",
    "        mode_train3 = np.load(f'../data/{situation}/For_ML/{mode}_train_2.npy')\n",
    "        mode_test = np.load(f'../data/{situation}/For_ML/{mode}_test.npy')\n",
    "        \n",
    "        X_train1, X_train2, X_train3 = mode_train1[:,:-1], mode_train2[:,:-1], mode_train3[:,:-1]\n",
    "        y_train1, y_train2, y_train3 = mode_train1[:,-1].astype(int).tolist(), mode_train2[:,-1].astype(int).tolist(), mode_train3[:,-1].astype(int).tolist()\n",
    "        X_test, y_test = mode_test[:,:-1], mode_test[:,-1].astype(int).tolist()\n",
    "        \n",
    "        \n",
    "        return X_train1, X_train2, X_train3, y_train1, y_train2, y_train3, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78b2355d-bc70-4a40-b797-16aff3623e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# majority and mean prediction votes\n",
    "def majority_vote(model1, model2, model3, mode_test):\n",
    "    pred1 = model1.predict_proba(mode_test)\n",
    "    pred2 = model2.predict_proba(mode_test)\n",
    "    pred3 = model3.predict_proba(mode_test)\n",
    "    \n",
    "    # mean\n",
    "    stack = pred1 + pred2 + pred3\n",
    "    stack_mean = stack/3\n",
    "    mean_vote = [np.argmax(i) for i in stack_mean]\n",
    "\n",
    "    major_binary_1 = [np.argmax(pred1[i]) for i in range(len(pred1))]\n",
    "    major_binary_2 = [np.argmax(pred2[i]) for i in range(len(pred2))]\n",
    "    major_binary_3 = [np.argmax(pred3[i]) for i in range(len(pred3))]\n",
    "\n",
    "    major_vote = []\n",
    "    for i in range(len(major_binary_1)):\n",
    "        tmp_result = []\n",
    "        tmp_result.append(major_binary_1[i])\n",
    "        tmp_result.append(major_binary_2[i])\n",
    "        tmp_result.append(major_binary_3[i])\n",
    "        result = np.mean(tmp_result)\n",
    "        if result > 0.5:\n",
    "            major_vote.append(1)\n",
    "        else:\n",
    "            major_vote.append(0)\n",
    "    \n",
    "    return mean_vote, major_vote\n",
    "\n",
    "# print out reports\n",
    "def report_save(y_true, y_pred, path, mode, name, label_names=None, *args, **kv):\n",
    "    result_path = f'../data/{path}/ML_predicted_results/{mode}_results'\n",
    "    # print the classification report here\n",
    "    report = classification_report(y_true, y_pred, target_names=label_names)\n",
    "    print(colored(f'\\n\\t\\t\\t\\t *** {name}_report ***:\\n\\n\\n', 'blue', attrs=['bold']), report)\n",
    "    MCC = matthews_corrcoef(y_true, y_pred)\n",
    "    print(f\"{name} MCC:\", MCC)\n",
    "\n",
    "    # create report dataframe\n",
    "    report_for_save = classification_report(y_true, y_pred, target_names=label_names, output_dict=True)\n",
    "    report_csv = pd.DataFrame(report_for_save).transpose()\n",
    "    report_csv['MCC'] = MCC\n",
    "\n",
    "    # style.background_gradient or highlight_max\n",
    "    report_styled = report_csv.style.background_gradient(subset=['precision', 'recall', 'f1-score'])\n",
    "    \n",
    "    # Save results\n",
    "    if not os.path.isdir(f'{result_path}'):\n",
    "        os.mkdir(f'{result_path}')\n",
    "\n",
    "    # export dataframe to .png\n",
    "    # dfi.export(report_styled, f'{result_path}/{name}_report.png')\n",
    "\n",
    "    report_csv.to_csv(f'{result_path}/{name}_report_save.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63b6a996-c22c-4f8e-b027-3808800cb310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_pred_for_imbalance(X_train, y_train, X_test, y_test, path, mode):\n",
    "    import lightgbm as lgb\n",
    "    # XGBoost\n",
    "    # xgb = XGBClassifier(n_estimators = 300, tree_method='gpu_hist', gpu_id=0)\n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(X_train, y_train)\n",
    "    y_xgb = xgb.predict(X_test)\n",
    "    report_save(y_test, y_xgb,path,mode = mode, name =  'XGBoost')\n",
    "    \n",
    "    # Catboost\n",
    "    cbt = CatBoostClassifier(iterations=10, learning_rate=0.09, depth=10)\n",
    "    cbt.fit(X_train, y_train)\n",
    "    y_cbt = cbt.predict(X_test)\n",
    "    report_save(y_test, y_cbt,path, mode = mode, name = 'CatBoost')\n",
    "    \n",
    "    # LightGBM\n",
    "    d_train=lgb.Dataset(X_train, label=y_train)\n",
    "    params={}\n",
    "    # params['learning_rate']=0.41282313322582176\n",
    "    params['learning_rate']=0.1\n",
    "    params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "    params['objective']='cross_entropy' #Binary target feature\n",
    "    params['metric']='binary_error' #metric for binary classification\n",
    "    params['max_depth']= 10\n",
    "    params['n_estimators'] = 300\n",
    "    # params['num_leaves'] = 34\n",
    "    # params['reg_lambda'] = 0.9557019573592245\n",
    "    # params['colsample_by_tree'] = 0.8506663985944544\n",
    "    lgb = lgb.train(params,d_train,300)\n",
    "    y_lgb = lgb.predict(X_test)\n",
    "    y_lgb=y_lgb.round(0)\n",
    "    y_lgb = y_lgb.astype(int)\n",
    "    report_save(y_test, y_lgb,path,mode = mode, name = 'LightGBM')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d00b04d-f44e-44e5-81e6-d85e4a60cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_pred_for_balance(X_train1, X_train2, X_train3, y_train1, y_train2, y_train3, X_test, y_test, path, mode):\n",
    "    \n",
    "    # XGBoost\n",
    "    # xgb_1 = XGBClassifier(n_estimators = 300, tree_method='gpu_hist', gpu_id=0)\n",
    "    xgb_1 = XGBClassifier()\n",
    "    xgb_2 = XGBClassifier()\n",
    "    xgb_3 = XGBClassifier()\n",
    "    xgb_1.fit(X_train1, y_train1)\n",
    "    xgb_2.fit(X_train2, y_train2)\n",
    "    xgb_3.fit(X_train3, y_train3)\n",
    "    xgb_mean_prodictions_binary, xgb_majority_vote_binary = majority_vote(xgb_1,xgb_2,xgb_3, X_test)\n",
    "    report_save(xgb_mean_prodictions_binary, y_test, path, mode,'XGBoost_mean')\n",
    "    report_save(xgb_majority_vote_binary, y_test, path, mode,'XGBoost_majority')\n",
    "\n",
    "    # Catboost\n",
    "    cbt_1 = CatBoostClassifier(iterations=10, learning_rate=0.09, depth=10)\n",
    "    cbt_2 = CatBoostClassifier(iterations=10, learning_rate=0.09, depth=10)\n",
    "    cbt_3 = CatBoostClassifier(iterations=10, learning_rate=0.09, depth=10)\n",
    "\n",
    "    cbt_1.fit(X_train1, y_train1)\n",
    "    cbt_2.fit(X_train2, y_train2)\n",
    "    cbt_3.fit(X_train3, y_train3)\n",
    "    cbt_mean_prodictions_binary, cbt_majority_vote_binary = majority_vote(cbt_1,cbt_2,cbt_3, X_test)\n",
    "    report_save(cbt_mean_prodictions_binary, y_test,path,mode, 'Catboost_mean')\n",
    "    report_save(cbt_majority_vote_binary, y_test,path, mode,'Catboost_majority')\n",
    "\n",
    "    # LightGBM\n",
    "    params={}\n",
    "    params['learning_rate']=0.1\n",
    "    params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
    "    params['objective']='cross_entropy' #Binary target feature\n",
    "    params['metric']='binary_error' #metric for binary classification\n",
    "    params['max_depth']= 10\n",
    "    params['n_estimators'] = 300\n",
    "    params['n_iter'] = 500\n",
    "\n",
    "    lgb_1 = lgb.LGBMClassifier(**params)\n",
    "    lgb_2 = lgb.LGBMClassifier(**params)\n",
    "    lgb_3 = lgb.LGBMClassifier(**params)\n",
    "\n",
    "    lgb_1.fit(X_train1, y_train1)\n",
    "    lgb_2.fit(X_train2, y_train2)\n",
    "    lgb_3.fit(X_train3, y_train3)\n",
    "\n",
    "    lgb_mean_prodictions_binary, lgb_majority_vote_binary = majority_vote(lgb_1,lgb_2,lgb_3, X_test)\n",
    "    report_save(lgb_mean_prodictions_binary, y_test,path, mode, 'LightGBM_mean')\n",
    "    report_save(lgb_majority_vote_binary, y_test,path, mode, 'LightGBM_majority')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea307380-8ee0-4a94-a0b6-f870b0c3e926",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train1_s4, X1_train2_s4, X1_train3_s4, y1_train1_s4, y1_train2_s4, y1_train3_s4, X1_test_s4, y1_test_s4 = train_test_dataset('balanced_diff_seq', 'mode1')\n",
    "X2_train1_s4, X2_train2_s4, X2_train3_s4, y2_train1_s4, y2_train2_s4, y2_train3_s4, X2_test_s4, y2_test_s4 = train_test_dataset('balanced_diff_seq', 'mode2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a64908ad-5d66-4bcf-8810-720446458c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train1_s4, X1_train2_s4, X1_train3_s4, y1_train1_s4, y1_train2_s4, y1_train3_s4, X1_test_s4, y1_test_s4 = X1_train1_s4[:100,:], X1_train2_s4[:100,:], X1_train3_s4[:100,:], y1_train1_s4[:100], y1_train2_s4[:100], y1_train3_s4[:100], X1_test_s4[:100,:], y1_test_s4[:100]\n",
    "X2_train2_s4, X2_train2_s4, X2_train3_s4, y2_train2_s4, y2_train2_s4, y2_train3_s4, X2_test_s4, y2_test_s4 = X2_train2_s4[:100,:], X2_train2_s4[:100,:], X2_train3_s4[:100,:], y2_train2_s4[:100], y2_train2_s4[:100], y2_train3_s4[:100], X2_test_s4[:100,:], y2_test_s4[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7846989-4e52-478b-aaf7-bc6f3648f168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/acetylcoa/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:17:40] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:17:41] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[19:17:41] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\u001b[1m\u001b[34m\n",
      "\t\t\t\t *** XGBoost_mean_report ***:\n",
      "\n",
      "\n",
      "\u001b[0m               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.87      0.53        39\n",
      "           1       0.55      0.10      0.17        61\n",
      "\n",
      "    accuracy                           0.40       100\n",
      "   macro avg       0.46      0.49      0.35       100\n",
      "weighted avg       0.48      0.40      0.31       100\n",
      "\n",
      "XGBoost_mean MCC: -0.046523221712558634\n",
      "\u001b[1m\u001b[34m\n",
      "\t\t\t\t *** XGBoost_majority_report ***:\n",
      "\n",
      "\n",
      "\u001b[0m               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.88      0.62        49\n",
      "           1       0.45      0.10      0.16        51\n",
      "\n",
      "    accuracy                           0.48       100\n",
      "   macro avg       0.47      0.49      0.39       100\n",
      "weighted avg       0.47      0.48      0.39       100\n",
      "\n",
      "XGBoost_majority MCC: -0.038999169712218056\n",
      "0:\tlearn: 0.6492546\ttotal: 84ms\tremaining: 756ms\n",
      "1:\tlearn: 0.5999003\ttotal: 259ms\tremaining: 1.04s\n",
      "2:\tlearn: 0.5586566\ttotal: 427ms\tremaining: 997ms\n",
      "3:\tlearn: 0.5201971\ttotal: 622ms\tremaining: 933ms\n",
      "4:\tlearn: 0.4907862\ttotal: 837ms\tremaining: 837ms\n",
      "5:\tlearn: 0.4592043\ttotal: 1.05s\tremaining: 702ms\n",
      "6:\tlearn: 0.4286813\ttotal: 1.27s\tremaining: 545ms\n",
      "7:\tlearn: 0.4040023\ttotal: 1.45s\tremaining: 364ms\n",
      "8:\tlearn: 0.3868560\ttotal: 1.62s\tremaining: 180ms\n",
      "9:\tlearn: 0.3638921\ttotal: 1.79s\tremaining: 0us\n",
      "0:\tlearn: 0.6483248\ttotal: 181ms\tremaining: 1.63s\n",
      "1:\tlearn: 0.6027540\ttotal: 383ms\tremaining: 1.53s\n",
      "2:\tlearn: 0.5624928\ttotal: 599ms\tremaining: 1.4s\n",
      "3:\tlearn: 0.5270508\ttotal: 800ms\tremaining: 1.2s\n",
      "4:\tlearn: 0.5009498\ttotal: 996ms\tremaining: 996ms\n",
      "5:\tlearn: 0.4682238\ttotal: 1.19s\tremaining: 793ms\n",
      "6:\tlearn: 0.4365648\ttotal: 1.37s\tremaining: 589ms\n",
      "7:\tlearn: 0.4083368\ttotal: 1.57s\tremaining: 393ms\n",
      "8:\tlearn: 0.3853661\ttotal: 1.69s\tremaining: 187ms\n",
      "9:\tlearn: 0.3596126\ttotal: 1.87s\tremaining: 0us\n",
      "0:\tlearn: 0.6483248\ttotal: 179ms\tremaining: 1.61s\n",
      "1:\tlearn: 0.6027540\ttotal: 390ms\tremaining: 1.56s\n",
      "2:\tlearn: 0.5624928\ttotal: 590ms\tremaining: 1.38s\n",
      "3:\tlearn: 0.5270508\ttotal: 779ms\tremaining: 1.17s\n",
      "4:\tlearn: 0.5009498\ttotal: 967ms\tremaining: 967ms\n",
      "5:\tlearn: 0.4682238\ttotal: 1.16s\tremaining: 776ms\n",
      "6:\tlearn: 0.4365648\ttotal: 1.36s\tremaining: 581ms\n",
      "7:\tlearn: 0.4083368\ttotal: 1.57s\tremaining: 394ms\n",
      "8:\tlearn: 0.3853661\ttotal: 1.67s\tremaining: 185ms\n",
      "9:\tlearn: 0.3596126\ttotal: 1.86s\tremaining: 0us\n",
      "\u001b[1m\u001b[34m\n",
      "\t\t\t\t *** Catboost_mean_report ***:\n",
      "\n",
      "\n",
      "\u001b[0m               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.88      0.56        42\n",
      "           1       0.55      0.10      0.17        58\n",
      "\n",
      "    accuracy                           0.43       100\n",
      "   macro avg       0.48      0.49      0.37       100\n",
      "weighted avg       0.49      0.43      0.34       100\n",
      "\n",
      "Catboost_mean MCC: -0.024606713274066026\n",
      "\u001b[1m\u001b[34m\n",
      "\t\t\t\t *** Catboost_majority_report ***:\n",
      "\n",
      "\n",
      "\u001b[0m               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.91      0.69        54\n",
      "           1       0.55      0.13      0.21        46\n",
      "\n",
      "    accuracy                           0.55       100\n",
      "   macro avg       0.55      0.52      0.45       100\n",
      "weighted avg       0.55      0.55      0.47       100\n",
      "\n",
      "Catboost_majority MCC: 0.060278260627726094\n",
      "[LightGBM] [Warning] num_iterations is set=500, n_iter=500 will be ignored. Current value: num_iterations=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/acetylcoa/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/acetylcoa/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
      "/Users/acetylcoa/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_iter` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m\n",
      "\t\t\t\t *** LightGBM_mean_report ***:\n",
      "\n",
      "\n",
      "\u001b[0m               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.79      0.48        39\n",
      "           1       0.27      0.05      0.08        61\n",
      "\n",
      "    accuracy                           0.34       100\n",
      "   macro avg       0.31      0.42      0.28       100\n",
      "weighted avg       0.30      0.34      0.24       100\n",
      "\n",
      "LightGBM_mean MCC: -0.24310021486421482\n",
      "\u001b[1m\u001b[34m\n",
      "\t\t\t\t *** LightGBM_majority_report ***:\n",
      "\n",
      "\n",
      "\u001b[0m               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.82      0.55        45\n",
      "           1       0.27      0.05      0.09        55\n",
      "\n",
      "    accuracy                           0.40       100\n",
      "   macro avg       0.34      0.44      0.32       100\n",
      "weighted avg       0.34      0.40      0.30       100\n",
      "\n",
      "LightGBM_majority MCC: -0.1959390020617817\n"
     ]
    }
   ],
   "source": [
    "ML_pred_for_balance(X1_train1_s4, X1_train2_s4, X1_train3_s4, y1_train1_s4, y1_train2_s4, y1_train3_s4, X1_test_s4, y1_test_s4, 'balanced_diff_seq', 'mode1')\n",
    "# ML_pred_for_balance(X2_train1_s4, X2_train2_s4, X2_train3_s4, y2_train1_s4, y2_train2_s4, y2_train3_s4, X2_test_s4, y2_test_s4, 'balanced_diff_seq', 'mode2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "68b74b68-2a80-4c97-8330-78fe489b1fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2048)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_train_s3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1869700e-1741-409e-8493-4ab3a41d3950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 79, 1: 21})"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(y1_train_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "959e3789-0054-4ba9-b064-bad33b7a34f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/acetylcoa/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:17:21] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\u001b[1m\u001b[34m\n",
      "\t\t\t\t *** XGBoost_report ***:\n",
      "\n",
      "\n",
      "\u001b[0m               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.79      0.83        89\n",
      "           1       0.10      0.18      0.12        11\n",
      "\n",
      "    accuracy                           0.72       100\n",
      "   macro avg       0.49      0.48      0.48       100\n",
      "weighted avg       0.80      0.72      0.76       100\n",
      "\n",
      "XGBoost MCC: -0.024324681077860674\n",
      "0:\tlearn: 0.6370890\ttotal: 148ms\tremaining: 1.33s\n",
      "1:\tlearn: 0.5914948\ttotal: 309ms\tremaining: 1.24s\n",
      "2:\tlearn: 0.5439692\ttotal: 355ms\tremaining: 828ms\n",
      "3:\tlearn: 0.4959131\ttotal: 519ms\tremaining: 778ms\n",
      "4:\tlearn: 0.4574382\ttotal: 706ms\tremaining: 706ms\n",
      "5:\tlearn: 0.4210059\ttotal: 896ms\tremaining: 598ms\n",
      "6:\tlearn: 0.3913038\ttotal: 1.1s\tremaining: 471ms\n",
      "7:\tlearn: 0.3573130\ttotal: 1.16s\tremaining: 290ms\n",
      "8:\tlearn: 0.3324870\ttotal: 1.34s\tremaining: 149ms\n",
      "9:\tlearn: 0.3107309\ttotal: 1.53s\tremaining: 0us\n",
      "\u001b[1m\u001b[34m\n",
      "\t\t\t\t *** CatBoost_report ***:\n",
      "\n",
      "\n",
      "\u001b[0m               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.79      0.83        89\n",
      "           1       0.10      0.18      0.12        11\n",
      "\n",
      "    accuracy                           0.72       100\n",
      "   macro avg       0.49      0.48      0.48       100\n",
      "weighted avg       0.80      0.72      0.76       100\n",
      "\n",
      "CatBoost MCC: -0.024324681077860674\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] [cross_entropy:Init]: (objective) labels passed interval [0, 1] check\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015261 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 70798\n",
      "[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 2048\n",
      "[LightGBM] [Info] [cross_entropy:BoostFromScore]: pavg = 0.210000 -> initscore = -1.324925\n",
      "[LightGBM] [Info] Start training from score -1.324925\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/acetylcoa/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "\u001b[1m\u001b[34m\n",
      "\t\t\t\t *** LightGBM_report ***:\n",
      "\n",
      "\n",
      "\u001b[0m               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.79      0.83        89\n",
      "           1       0.10      0.18      0.12        11\n",
      "\n",
      "    accuracy                           0.72       100\n",
      "   macro avg       0.49      0.48      0.48       100\n",
      "weighted avg       0.80      0.72      0.76       100\n",
      "\n",
      "LightGBM MCC: -0.024324681077860674\n"
     ]
    }
   ],
   "source": [
    "ML_pred_for_imbalance(X1_train_s3, y1_train_s3, X1_test_s3, y1_test_s3,'imbalance_diff_seq', mode = 'mode1')\n",
    "# ML_pred_for_imbalance(X2_train_s3, y2_train_s3, X2_test_s3, y2_test_s3,'imbalance_diff_seq', mode = 'mode2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e864e54d-7c79-4a4f-977f-68d3541e5ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:33:36] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:33:36] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[17:33:36] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=&#x27;()&#x27;, n_estimators=100, n_jobs=10,\n",
       "              num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method=&#x27;exact&#x27;, validate_parameters=1, verbosity=None)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints=&#x27;&#x27;, learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=&#x27;()&#x27;, n_estimators=100, n_jobs=10,\n",
       "              num_parallel_tree=1, predictor=&#x27;auto&#x27;, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method=&#x27;exact&#x27;, validate_parameters=1, verbosity=None)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=10,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_1 = XGBClassifier()\n",
    "xgb_2 = XGBClassifier()\n",
    "xgb_3 = XGBClassifier()\n",
    "xgb_1.fit(X1_train1_s2, y1_train1_s2)\n",
    "xgb_2.fit(X1_train2_s2, y1_train2_s2)\n",
    "xgb_3.fit(X1_train3_s2, y1_train3_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3ccb5d42-4398-4444-9e42-4ca80e5891c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = xgb_1.predict_proba(X1_test_s2)\n",
    "pred2 = xgb_2.predict_proba(X1_test_s2)\n",
    "pred3 = xgb_3.predict_proba(X1_test_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d2a70fa5-c98e-476a-be7d-367a174b1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "stack = pred1 + pred2 + pred3\n",
    "stack_mean = stack/3\n",
    "mean_vote = [np.argmax(i) for i in stack_mean]\n",
    "\n",
    "major_binary_1 = [np.argmax(pred1[i]) for i in range(len(pred1))]\n",
    "major_binary_2 = [np.argmax(pred2[i]) for i in range(len(pred2))]\n",
    "major_binary_3 = [np.argmax(pred3[i]) for i in range(len(pred3))]\n",
    "\n",
    "major_result = []\n",
    "for i in range(len(result_1)):\n",
    "    tmp_result = []\n",
    "    tmp_result.append(major_binary_1[i])\n",
    "    tmp_result.append(major_binary_2[i])\n",
    "    tmp_result.append(major_binary_3[i])\n",
    "    result = np.mean(tmp_result)\n",
    "    if result > 0.5:\n",
    "        major_result.append(1)\n",
    "    else:\n",
    "        major_result.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2a22abfa-28b9-4d61-9e9e-3713504ce66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "major_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7a80a2b3-c3cd-496d-b23d-71a81d4194ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "major_binary_1 = [np.argmax(pred1[i]) for i in range(len(pred1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a89d8fa0-7b36-4608-bbdd-a14c728803be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "major_binary_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "82c61289-975e-4eba-ac8b-d6dbcd65c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(pred):\n",
    "    result_1 = []\n",
    "    for i in range(len(pred)):\n",
    "        result = np.argmax(pred[i])\n",
    "        result_1.append(result)\n",
    "    return result_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5f3d5139-b07f-49ac-bcad-000073791bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1 = get_result(pred1)\n",
    "result_2 = get_result(pred2)\n",
    "result_3 = get_result(pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2c77be91-6050-4190-b1d6-22a871bf72df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "556f8f42-1abb-4432-9b3c-2fcb08bd44fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4ced67e0-3865-4d43-a592-5876e545a39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 1, 1]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "04decb2d-3239-4f7f-ace9-6efbd7b32436",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_result = []\n",
    "for i in range(len(result_1)):\n",
    "    tmp_result = []\n",
    "    tmp_result.append(result_1[i])\n",
    "    tmp_result.append(result_2[i])\n",
    "    tmp_result.append(result_3[i])\n",
    "    result = np.mean(tmp_result)\n",
    "    if result > 0.5:\n",
    "        max_result.append(1)\n",
    "    else:\n",
    "        max_result.append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cc25c606-69d9-43e0-b922-3d7f18bcebea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 0, 1, 1]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7a70d12-fdd6-4100-a91d-725afb147840",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_preds = np.vstack([pred1, pred2, pred3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ba5d785-def7-4ed9-9c18-5aaf8f5c6f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2089d0d0-9b60-496f-a21e-4a583d2dbde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39999998, 0.6       ],\n",
       "       [0.39999998, 0.6       ],\n",
       "       [0.39999998, 0.6       ],\n",
       "       [0.39999998, 0.6       ],\n",
       "       [0.39999998, 0.6       ],\n",
       "       [0.27574146, 0.72425854],\n",
       "       [0.27574146, 0.72425854],\n",
       "       [0.27574146, 0.72425854],\n",
       "       [0.27574146, 0.72425854],\n",
       "       [0.27574146, 0.72425854],\n",
       "       [0.27574146, 0.72425854],\n",
       "       [0.27574146, 0.72425854],\n",
       "       [0.27574146, 0.72425854],\n",
       "       [0.27574146, 0.72425854],\n",
       "       [0.27574146, 0.72425854]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b307cf4e-304c-44a0-91e3-993b78edacfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39999998 0.6       ]\n",
      "patho\n"
     ]
    }
   ],
   "source": [
    "result = np.argmax(b)\n",
    "# (stacked_preds > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "914be5b6-e329-4acb-a810-c63d4234684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_predictions_proba = stacked_preds.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f6bdb-a0db-4091-bd86-9445b1654126",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.argmax(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1025e03-1359-4144-9b8f-8864c19b849d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.31716102, 0.68283904], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_predictions_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "99a91f5d-47ae-4fd8-94bd-0a2ef9e4028f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 1]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_test_s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eafa280a-74d2-47ec-92bc-48df8923a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_predictions_binary = (mean_predictions_proba > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "566ea5c8-55c3-429f-abf4-dbb40c322472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_predictions_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89052565-615f-4d8e-a8f0-baa43714ae7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adfb019-60d7-4d05-80c7-f77d4ea6b1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2142f648-4b76-4090-97e6-5850a88799c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "76feca60-4db0-4ded-bffd-f68695d0ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_vote_proba  = binarized_predictions.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d55b42b-60db-4f7c-9461-a6884ca0766c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majority_vote_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "87dea8dd-fcd0-4bbb-b959-644da083d3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majority_vote_binary  = (majority_vote_proba > 0.5).astype(int)\n",
    "majority_vote_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43113b5f-bfc5-4717-98df-52aeeccf93ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# majority and mean prediction votes\n",
    "def majority_vote(model1, model2, model3, mode_test):\n",
    "    pred1 = model1.predict_proba(mode_test)\n",
    "    pred2 = model2.predict_proba(mode_test)\n",
    "    pred3 = model3.predict_proba(mode_test)\n",
    "    \n",
    "    stacked_preds = np.vstack([pred1, pred2, pred3])\n",
    "\n",
    "    # Take mean of all three models # use this for AUC ROC, prec-recall AUC etc.\n",
    "    mean_predictions_proba = stacked_preds.mean(axis=0) # might be axis =1 just make sure that the output shape is n_predictions long\n",
    "\n",
    "    # use this for f1 score, precision, recall, MCC etc.\n",
    "    mean_predictions_binary = (mean_predictions_proba > 0.5).astype(int)\n",
    "\n",
    "    # convert the mean predictions of each model to binary\n",
    "    binarized_predictions = (stacked_preds > 0.5).astype(int)\n",
    "\n",
    "    # Take the average of the binary votes #use this for AUC ROC, prec-recall AUC etc.\n",
    "    majority_vote_proba  = binarized_predictions.mean(axis=0) # might be axis =1 just make sure that the output shape is n_predictions long\n",
    "\n",
    "    # Convert the majority vote to binary # use this for f1 score, precision, recall, MCC etc.\n",
    "    majority_vote_binary  = (majority_vote_proba > 0.5).astype(int)\n",
    "    \n",
    "    return mean_predictions_binary, majority_vote_binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949bed57-0607-4bf8-adda-68157a61f7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2545c293-1292-4bad-9cd9-faa6cb22e5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05a8dde4-fae5-4a5d-870d-618c3d648888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in /Users/acetylcoa/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c02b27-43eb-40cd-a501-62523126bdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_for_downstream():\n",
    "    path = \"../data/pcm1/\"\n",
    "    concat = []\n",
    "    for pkl in os.listdir(path):\n",
    "        if(\".pkl\" in pkl):\n",
    "            file_path = path + pkl\n",
    "            with open(file_path, 'rb') as file:\n",
    "                y = pickle.load(file)\n",
    "                concat += y\n",
    "    data_y = []\n",
    "    data_X = []\n",
    "    for i in range(len(concat)):\n",
    "        data_X.append(concat[i]['x'][0])\n",
    "        # data_y.append(int(concat[i]['label']))\n",
    "    data_X = np.array(data_X)\n",
    "    return data_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "600f85c7-114a-4080-9d39-4ec40345871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b3cc150e-65a4-4343-b5da-fed120bfaed2",
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[20:57:53] /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/include/xgboost/json.h:73: Invalid cast, from Integer to Boolean\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000000176794394 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x00000001767bc7cc xgboost::JsonBoolean const* xgboost::Cast<xgboost::JsonBoolean const, xgboost::Value const>(xgboost::Value const*) + 660\n  [bt] (2) 3   libxgboost.dylib                    0x00000001768abc98 xgboost::RegTree::LoadModel(xgboost::Json const&) + 4396\n  [bt] (3) 4   libxgboost.dylib                    0x0000000176828300 xgboost::gbm::GBTreeModel::LoadModel(xgboost::Json const&) + 720\n  [bt] (4) 5   libxgboost.dylib                    0x00000001768160c0 xgboost::gbm::GBTree::LoadModel(xgboost::Json const&) + 448\n  [bt] (5) 6   libxgboost.dylib                    0x000000017682b3c0 xgboost::LearnerIO::LoadModel(xgboost::Json const&) + 1368\n  [bt] (6) 7   libxgboost.dylib                    0x000000017679d414 XGBoosterLoadModel + 1068\n  [bt] (7) 8   libffi.8.dylib                      0x0000000104e0c04c ffi_call_SYSV + 76\n  [bt] (8) 9   libffi.8.dylib                      0x0000000104e0974c ffi_call_int + 1208\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [67]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model_xgb_2 \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_xgb_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/model/XGBoost_model.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/xgboost/sklearn.py:599\u001b[0m, in \u001b[0;36mXGBModel.load_model\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Booster\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m Booster({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs})\n\u001b[0;32m--> 599\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m meta_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_booster()\u001b[38;5;241m.\u001b[39mattr(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscikit_learn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meta_str \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;66;03m# FIXME(jiaming): This doesn't have to be a problem as most of the needed\u001b[39;00m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;66;03m# information like num_class and objective is in Learner class.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/xgboost/core.py:2169\u001b[0m, in \u001b[0;36mBooster.load_model\u001b[0;34m(self, fname)\u001b[0m\n\u001b[1;32m   2165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, (\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike)):\n\u001b[1;32m   2166\u001b[0m     \u001b[38;5;66;03m# assume file name, cannot use os.path.exist to check, file can be\u001b[39;00m\n\u001b[1;32m   2167\u001b[0m     \u001b[38;5;66;03m# from URL.\u001b[39;00m\n\u001b[1;32m   2168\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(fname))\n\u001b[0;32m-> 2169\u001b[0m     \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterLoadModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2170\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2171\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mbytearray\u001b[39m):\n\u001b[1;32m   2172\u001b[0m     buf \u001b[38;5;241m=\u001b[39m fname\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/xgboost/core.py:218\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [20:57:53] /Users/runner/miniforge3/conda-bld/xgboost-split_1645117899018/work/include/xgboost/json.h:73: Invalid cast, from Integer to Boolean\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000000176794394 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x00000001767bc7cc xgboost::JsonBoolean const* xgboost::Cast<xgboost::JsonBoolean const, xgboost::Value const>(xgboost::Value const*) + 660\n  [bt] (2) 3   libxgboost.dylib                    0x00000001768abc98 xgboost::RegTree::LoadModel(xgboost::Json const&) + 4396\n  [bt] (3) 4   libxgboost.dylib                    0x0000000176828300 xgboost::gbm::GBTreeModel::LoadModel(xgboost::Json const&) + 720\n  [bt] (4) 5   libxgboost.dylib                    0x00000001768160c0 xgboost::gbm::GBTree::LoadModel(xgboost::Json const&) + 448\n  [bt] (5) 6   libxgboost.dylib                    0x000000017682b3c0 xgboost::LearnerIO::LoadModel(xgboost::Json const&) + 1368\n  [bt] (6) 7   libxgboost.dylib                    0x000000017679d414 XGBoosterLoadModel + 1068\n  [bt] (7) 8   libffi.8.dylib                      0x0000000104e0c04c ffi_call_SYSV + 76\n  [bt] (8) 9   libffi.8.dylib                      0x0000000104e0974c ffi_call_int + 1208\n\n"
     ]
    }
   ],
   "source": [
    "model_xgb_2 = xgb.XGBClassifier()\n",
    "model_xgb_2.load_model(\"../data/model/XGBoost_model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef9dc79d-2827-4716-93ec-e32dfff2a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catboost load\n",
    "cbt = CatBoostClassifier()\n",
    "file_name = \"../data/model/CatBoost_model\"\n",
    "cbt = cbt.load_model(file_name, format = 'cbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7e1e34c6-7223-426d-85a2-ae455394dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM load\n",
    "# joblib.load('lgb.pkl')\n",
    "lgb = joblib.load(\"../data/model/LightGBM_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ff101787-1ccb-4238-a0e1-626d7314c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lgb = lgb.predict(data_X).round(0).astype(int)\n",
    "# y_lgb=y_lgb.round(0)\n",
    "# y_lgb = y_lgb.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "576897b7-c61d-4ea6-9ed6-9584e3849440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a51c996-8559-4c33-ba90-790fe3e40222",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = data_for_downstream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d856a7e9-5e4c-4e4c-adda-8867cc50d1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 2048)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2911a14-822f-49de-b233-a4ce41ff6a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pred = cbt.predict(data_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66b00bdc-79ec-410e-8368-69aaeb0faf48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcc7e699-74ef-45e4-9532-051034671cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y1_test_s3, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a556f91f-fc1f-4454-bf62-59ba25370660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34m\n",
      "\t\t\t\t *** Catboost_report ***:\n",
      "\n",
      "\n",
      "\u001b[0m               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.98      0.87      3710\n",
      "           1       0.23      0.02      0.03      1023\n",
      "\n",
      "    accuracy                           0.77      4733\n",
      "   macro avg       0.51      0.50      0.45      4733\n",
      "weighted avg       0.66      0.77      0.69      4733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name = 'Catboost'\n",
    "print(colored(f'\\n\\t\\t\\t\\t *** {name}_report ***:\\n\\n\\n', 'blue', attrs=['bold']), report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b17e24-17cc-4339-b128-5af097d4bea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
